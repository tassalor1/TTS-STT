{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3062a6e5-bf21-48db-a631-daf013a1b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2780a3d2-feee-412a-b390-298a268572b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m file_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m audio_folder:\n\u001b[0;32m      8\u001b[0m         \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Transcribe audio to text\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudioFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Record the entire audio file\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use Google's ASR\u001b[39;00m\n",
      "File \u001b[1;32mD:\\coding\\ai_assistant\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:241\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis audio source is already inside a context manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# attempt to read the file as WAV\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m \u001b[43mwave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_or_fileobject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (wave\u001b[38;5;241m.\u001b[39mError, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\wave.py:631\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    629\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWave_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_write(f)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\wave.py:279\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# else, assume it is an open file object already\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a'"
     ]
    }
   ],
   "source": [
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load an audio folder\n",
    "audio_folder = \"ai_assistant\\\\training_data\"\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(\".mp3\")\n",
    "               \n",
    "for i in audio_folder:\n",
    "\n",
    "               audio_path = os.path.join(audio_folder, audio\n",
    "               \n",
    "               \n",
    "            \n",
    "                # Transcribe audio to text\n",
    "                with sr.AudioFile(i) as source:\n",
    "                    audio = recognizer.record(source)  # Record the entire audio file\n",
    "                    transcription = recognizer.recognize_google(audio)  # Use Google's ASR\n",
    "                    \n",
    "                # file_count += 1\n",
    "                # Save the transcription to a text file\n",
    "                transcription_file = \"transcription_{i}.txt\"\n",
    "                with open(transcription_file, \"w\") as file:\n",
    "                    file.write(transcription)\n",
    "                \n",
    "                # Print a confirmation message\n",
    "                print(f\"Transcription {i} saved to {transcription_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffa9a1-2072-4b8b-aeb2-d01950e905c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Audiobook\n",
    "audio = AudioSegment.from_file(\"audiobook.mp3\")\n",
    "\n",
    "# Step 2: Text-Audio Alignment (using Aeneas )\n",
    "# Assume the alignments are obtained and saved in a list of tuples: alignments\n",
    "# Each tuple contains the start and end times (in milliseconds) for each audio chunk\n",
    "# alignments = [(start1, end1), (start2, end2), ...]\n",
    "\n",
    "# Also, assume transcription.txt contains the corresponding text for each audio chunk\n",
    "# Each line in transcription.txt corresponds to an audio chunk\n",
    "\n",
    "# Step 3: Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train('--input=transcription.txt --model_prefix=m --vocab_size=2000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# Step 4: Preprocess Data\n",
    "audio_chunks = [audio[start:end] for start, end in alignments]\n",
    "\n",
    "# Read the text data from transcription.txt\n",
    "with open('transcription.txt', 'r') as file:\n",
    "    texts = [line.strip() for line in file]\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_texts = [sp.encode(text, out_type=str) for text in texts]\n",
    "\n",
    "# Step 5: Feature Engineering\n",
    "def extract_features(audio_chunk):\n",
    "    samples = np.array(audio_chunk.get_array_of_samples())\n",
    "    spectrogram = librosa.feature.melspectrogram(y=samples, sr=audio_chunk.frame_rate)\n",
    "    return spectrogram\n",
    "\n",
    "spectrograms = [extract_features(audio_chunk) for audio_chunk in audio_chunks]\n",
    "\n",
    "# Step 6: Create Dataset\n",
    "dataset = list(zip(spectrograms, texts, tokenized_texts))\n",
    "\n",
    "# Step 7: Model Selection and Training\n",
    "# Choose a TTS model architecture (e.g., Tacotron 2)\n",
    "# Set up the training environment and train the model on the prepared dataset\n",
    "# The exact steps will depend on the TTS framework and model you choose\n",
    "\n",
    "# Step 8: Evaluation\n",
    "# Evaluate the performance of your TTS model\n",
    "# The evaluation steps will also depend on the TTS framework and model you choose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526b7d65-db2a-4097-af41-36389369490d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbark_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarkConfig\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bark\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "from TTS.tts.configs.bark_config import BarkConfig\n",
    "from TTS.tts.models.bark import Barkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a98f08-2917-46f4-8d97-8c3bb03120c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BarkProcessor.from_pretrained(\"suno/bark\")\n",
    "\n",
    "\n",
    "train_dataset = datasets.load_dataset(\"\")\n",
    "\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    text = batch[\"text\"]\n",
    "    inputs = processor(text, audio, return_tensors=\"pt\", padding=True)\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data)\n",
    "\n",
    "model = BarkForPreTraining.from_pretrained(\"suno/bark\")\n",
    "\n",
    "# training params\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# initialise the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=lambda data: {\"input_values\": data[\"input_values\"], \"labels\": data[\"labels\"]},\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-venv",
   "language": "python",
   "name": "ai-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
