{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3062a6e5-bf21-48db-a631-daf013a1b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780a3d2-feee-412a-b390-298a268572b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## take audio and transcribe audio to text file\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "\n",
    "audio_folder = \"D:/coding/ai_assistant/training_data\"\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(\".mp3\")]\n",
    "transcription_folder = \"D:/coding/ai_assistant/transcriptions\" \n",
    "\n",
    "for audio_file in audio_files:\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    audio_path = audio_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "    #convert mp3 to wav for sr libary\n",
    "    audio = AudioSegment.from_mp3(audio_path)\n",
    "    wav_path = audio_path.replace(\".mp3\", \".wav\")\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    \n",
    "    # transcribe audio to text\n",
    "    with sr.AudioFile(wav_path) as source:\n",
    "        audio = recognizer.record(source)  # record the  audio file\n",
    "        transcription = recognizer.recognize_sphinx(audio)  \n",
    "    \n",
    "    # save the transcription to a text file\n",
    "    transcription_file_name = f\"transcription_{audio_file}.txt\"\n",
    "    transcription_file_path = os.path.join(transcription_folder, transcription_file_name)\n",
    "\n",
    "    with open(transcription_file_path, \"w\") as file:\n",
    "        file.write(transcription)\n",
    "\n",
    "    print(f\"Transcription {audio_file} saved to {transcription_file_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb94b95-8f4f-4a4a-a691-b2b7026aca59",
   "metadata": {},
   "source": [
    "run docker container of the gentle alignment libary\n",
    "\n",
    "docker pull lowerquality/gentle\n",
    "\n",
    "docker run -P lowerquality/gentle\n",
    "\n",
    "#check which port\n",
    "docker ps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958643ac-0e37-4227-bc23-976acaf85316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_00_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_01_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_02_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_03_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_04_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_05_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_06_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_07_weekley_64kb.json\n",
      "Alignment data saved to D:/coding/ai_assistant/audio_alignment\\alignment_twentyyearsonhorseback_08_weekley_64kb.json\n"
     ]
    }
   ],
   "source": [
    "url = 'http://localhost:32768/transcriptions?async=false'\n",
    "audio_folder = \"D:/coding/ai_assistant/audio_data\"\n",
    "transcript_folder = \"D:/coding/ai_assistant/transcriptions\"\n",
    "output_folder = \"D:/coding/ai_assistant/audio_alignment\"\n",
    "\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    # get the transcript filename from the audio filename\n",
    "    transcript_file = \"transcription_\" + audio_file.replace(\".wav\", \".mp3\") + \".txt\"\n",
    "    \n",
    "    # full paths to the audio and transcript files\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    transcript_path = os.path.join(transcript_folder, transcript_file)\n",
    " \n",
    "    if os.path.exists(transcript_path):\n",
    "        # request to Gentle api\n",
    "        with open(audio_path, 'rb') as audio, open(transcript_path, 'r') as transcript:\n",
    "            response = requests.post(url, files={'audio': audio, 'transcript': transcript})\n",
    "     \n",
    "        if response.status_code == 200:\n",
    "            alignment_data = response.json()\n",
    "            \n",
    "            # output file path\n",
    "            output_file = f\"alignment_{audio_file.replace('.wav', '.json')}\"\n",
    "            output_path = os.path.join(output_folder, output_file)\n",
    "        \n",
    "    \n",
    "            with open(output_path, 'w') as outfile:\n",
    "                json.dump(alignment_data, outfile, indent=4)\n",
    "            \n",
    "            print(f\"Alignment data saved to {output_path}\")\n",
    "        else:\n",
    "            print(f'Failed to process alignment: {response.status_code}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c20246-fe53-4d18-bd1a-f3358a737235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata.csv file has been created at D:/coding/ai_assistant/dataset/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "txt_folder = \"D:/coding/ai_assistant/dataset/transcriptions\"\n",
    "wavs_folder = \"D:/coding/ai_assistant/dataset/wav\"\n",
    "output_csv_file = \"D:/coding/ai_assistant/dataset/metadata.csv\"  # Fixed the path\n",
    "\n",
    "def get_transcription(txt_file_path):\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "# Create metadata.csv file\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['wav_file', 'transcription']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='|')\n",
    "\n",
    "    # Loop through each wav file in the wavs folder\n",
    "    for wav_file_name in os.listdir(wavs_folder):\n",
    "        wav_file_path = os.path.join(wavs_folder, wav_file_name)\n",
    "        txt_file_name = \"transcription_\" + wav_file_name.replace(\".wav\", \".txt\")  # Fixed the extension to .txt\n",
    "        txt_file_path = os.path.join(txt_folder, txt_file_name)\n",
    "\n",
    "        # Check if the corresponding txt file exists\n",
    "        if os.path.exists(txt_file_path):\n",
    "            transcription = get_transcription(txt_file_path)\n",
    "            writer.writerow({'wav_file': wav_file_path, 'transcription': transcription})  # Use wav_file_path to include the full path\n",
    "        else:\n",
    "            print(f\"No matching txt file found for {wav_file_name}\")\n",
    "\n",
    "print(f'csv file has been created at {output_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc3098e-dbd3-4b9e-930a-1d4d7f5e0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert and split data \n",
    "csv_path = \"D:/coding/ai_assistant/dataset/metadata.csv\"\n",
    "data = pd.read_csv(csv_path, delimiter='|', header=None, names=['wav_file', 'transcription'])\n",
    "\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_txt = \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/preprocessed_data/MyDataset/metadata.txt\"\n",
    "train_txt = \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/preprocessed_data/MyDataset/train.txt\"\n",
    "val_txt = \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/preprocessed_data/MyDataset/val.txt\"\n",
    "\n",
    "\n",
    "train_data.to_csv(train_txt, sep='|', index=False, header=False)\n",
    "val_data.to_csv(val_txt, sep='|', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dda445-0bf0-487b-8573-3fc812ac468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## update preprocess yaml file\n",
    "def train_yaml(file_path, new_values):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    for key, value in new_values.items():\n",
    "        if key in config:\n",
    "            config[key].update(value)\n",
    "        else:\n",
    "            config[key] = value\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.safe_dump(config, file)\n",
    "\n",
    "# new values\n",
    "new_values_train = {\n",
    "    'path': {\n",
    "        'ckpt_path': \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/output/ckpt/MyDataset\",\n",
    "        'log_path': \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/output/log/MyDataset\",\n",
    "        'result_path': \"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/output/result/MyDataset\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update the train.yaml file\n",
    "train_yaml(\"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/config/LJSpeech/train.yaml\", new_values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a149eaf7-a2ea-4fb1-8d2f-3a2ec654771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## update preprocess yaml file\n",
    "def pre_yaml(file_path, new_values):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    for key, value in new_values.items():\n",
    "        if key in config:\n",
    "            config[key].update(value)\n",
    "        else:\n",
    "            config[key] = value\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.safe_dump(config, file)\n",
    "\n",
    "# new values \n",
    "new_values_preprocess = {\n",
    "    'path': {\n",
    "        'corpus_path': \"D:/coding/ai_assistant/dataset\",\n",
    "        'raw_path': \"./raw_data/MyDataset\",\n",
    "        'preprocessed_path': \"./preprocessed_data/MyDataset\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_yaml(\"D:/coding/ai_assistant/TensorFlowTTS/FastSpeech2/config/LJSpeech/preprocess.yaml\", new_values_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b67bb2-41d7-40e4-b6d7-e991554ca637",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', 'preprocess.py', 'config/LJSpeech/preprocess.yaml']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocess.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/LJSpeech/preprocess.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the command\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[1;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', 'preprocess.py', 'config/LJSpeech/preprocess.yaml']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "command = [\n",
    "    'python',\n",
    "    'preprocess.py',\n",
    "    'config/LJSpeech/preprocess.yaml'\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "subprocess.run(command, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f2a7a-004b-422d-b277-a9d0f4d16c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-venv",
   "language": "python",
   "name": "ai-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
